# ğŸ“š RAG Chatbot pour Roman "Wa7ch" - SystÃ¨me d'Extraction et de Questions-RÃ©ponses en Arabe

<div align="center">

![Python Version](https://img.shields.io/badge/python-3.11%2B-blue)
![LangChain](https://img.shields.io/badge/LangChain-compatible-orange)
![Streamlit](https://img.shields.io/badge/Streamlit-UI-purple)
![Ollama](https://img.shields.io/badge/Ollama-Local%20LLM-green)

*Un systÃ¨me RAG complet pour extraire et interroger le contenu du roman "Wa7ch" en arabe*

[FonctionnalitÃ©s](#-fonctionnalitÃ©s) â€¢ [Installation](#-installation) â€¢ [DÃ©marrage Rapide](#-dÃ©marrage-rapide) â€¢ [Architecture](#-architecture) â€¢ [Documentation](#-documentation) â€¢ [Contact](#-contact)

</div>

---

## ğŸ“‹ Table des MatiÃ¨res

- [AperÃ§u du Projet](#-aperÃ§u-du-projet)
- [FonctionnalitÃ©s](#-fonctionnalitÃ©s)
- [Structure du Projet](#-structure-du-projet)
- [PrÃ©requis](#-prÃ©requis)
- [Installation](#-installation)
- [DÃ©marrage Rapide](#-dÃ©marrage-rapide)
- [Architecture du SystÃ¨me](#-architecture-du-systÃ¨me)
- [Explication DÃ©taillÃ©e du Code](#-explication-dÃ©taillÃ©e-du-code)
- [Configuration](#-configuration)
- [DÃ©pannage](#-dÃ©pannage)
- [Contact et Support](#-contact-et-support)

---

## ğŸŒŸ AperÃ§u du Projet

Ce projet implÃ©mente un systÃ¨me **RAG (Retrieval-Augmented Generation)** complet pour le roman arabe "Wa7ch". Il comprend deux composants principaux :

1. **ğŸ“Š Extraction et Indexation** : Conversion d'un PDF en arabe vers une base de donnÃ©es vectorielle
2. **ğŸ’¬ Chatbot Intelligent** : Interface de conversation pour poser des questions sur le roman

### Objectifs du Projet

- ğŸ¯ **Extraire efficacement** le texte arabe d'un PDF avec prÃ©servation de la structure
- ğŸ—ï¸ **CrÃ©er une base vectorielle** pour la recherche sÃ©mantique
- ğŸ¤– **ImplÃ©menter un chatbot** avec interface utilisateur intuitive
- ğŸ”’ **Maintenir la confidentialitÃ©** avec des modÃ¨les locaux (Ollama)
- ğŸŒ **Support optimal de l'arabe** pour le traitement du langage naturel

---

## âœ¨ FonctionnalitÃ©s

### ğŸ“¥ Extraction de Documents

- âœ… **Extraction de PDF en arabe** avec LlamaParse
- âœ… **PrÃ©servation des tableaux et structures** complexes
- âœ… **Conversion en Markdown** structurÃ©
- âœ… **Support asynchrone** pour le traitement de fichiers

### ğŸ—ƒï¸ Base de DonnÃ©es Vectorielle

- âœ… **DÃ©coupage intelligent** en paragraphes
- âœ… **Embeddings locaux** avec Ollama (mxbai-embed-large)
- âœ… **Stockage persistant** avec ChromaDB
- âœ… **Recherche sÃ©mantique** optimisÃ©e pour l'arabe

### ğŸ’¬ Chatbot Intelligent

- âœ… **Interface Web moderne** avec Streamlit
- âœ… **Design responsive** et adaptatif
- âœ… **Questions suggÃ©rÃ©es** pour une expÃ©rience utilisateur amÃ©liorÃ©e
- âœ… **Historique de conversation** persistant
- âœ… **Recherche RAG** en temps rÃ©el

### ğŸ”§ FonctionnalitÃ©s Techniques

- âœ… **SÃ©curisation des clÃ©s API** (avec avertissement)
- âœ… **Gestion des erreurs** robuste
- âœ… **Logs dÃ©taillÃ©s** pour le dÃ©bogage
- âœ… **Configuration flexible** via variables d'environnement

---

## ğŸ“ Structure du Projet

```
rag-roman-wa7ch/
â”‚
â”œâ”€â”€ ğŸ““ dataembeddings.ipynb           # Notebook Jupyter pour l'extraction et indexation
â”œâ”€â”€ ğŸ¤– chatbotmain.py                 # Application chatbot Streamlit
â”‚
â”œâ”€â”€ ğŸ“„ Wa7ch.pdf                      # Roman original (PDF)
â”œâ”€â”€ ğŸ“ Wa7ch.md                       # Texte extrait en Markdown
â”‚
â”œâ”€â”€ ğŸ“ philo_db/                      # Base de donnÃ©es vectorielle ChromaDB
â”‚   â”œâ”€â”€ chroma.sqlite3
â”‚   â”œâ”€â”€ chroma-collections.parquet
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ ğŸ“ rag/                           # Alternative pour la base vectorielle
â”‚   â””â”€â”€ philo_db/
â”‚
â”œâ”€â”€ ğŸ“‹ requirements.txt               # DÃ©pendances Python
â””â”€â”€ ğŸ“– README.md                      # Documentation complÃ¨te
```

---

## ğŸ”§ PrÃ©requis

### SystÃ¨me d'Exploitation
- Windows 10/11, macOS 10.14+, ou Linux Ubuntu 18.04+
- 8GB RAM minimum (16GB recommandÃ©)
- 2GB d'espace disque libre

### Logiciels
- Python 3.11 ou supÃ©rieur
- pip (gestionnaire de packages Python)
- Git (pour cloner le repository)
- Ollama (pour les modÃ¨les locaux)

---

## ğŸš€ Installation

### Ã‰tape 1 : Cloner le Repository

```bash
git clone https://github.com/Oussama-fahim/chatbot-to-answer-questions-about-my-story-wa7ch.git
cd rag-roman-wa7ch
```

### Ã‰tape 2 : CrÃ©er un Environnement Virtuel

```bash
# Windows
python -m venv venv
venv\Scripts\activate

# macOS/Linux
python3 -m venv venv
source venv/bin/activate
```

### Ã‰tape 3 : Installer les DÃ©pendances

```bash
pip install -r requirements.txt
```

### Ã‰tape 4 : Installer et Configurer Ollama

```bash
# TÃ©lÃ©charger Ollama depuis https://ollama.ai/
# Installer le modÃ¨le d'embedding
ollama pull mxbai-embed-large

# Installer le modÃ¨le de langage
ollama pull llama3.1
```

### Ã‰tape 5 : Configurer les ClÃ©s API

```bash
# CrÃ©er un fichier .env
echo "LLAMA_CLOUD_API_KEY=votre_clÃ©_api_ici" > .env
```

**âš ï¸ Important** : Remplacez la clÃ© API dans le notebook par une variable d'environnement pour la sÃ©curitÃ©.

---

## âš¡ DÃ©marrage Rapide

### Phase 1 : Extraction et Indexation

```bash
# Lancer Jupyter Notebook
jupyter notebook dataembeddings.ipynb

# ExÃ©cuter toutes les cellules dans l'ordre :
# 1. Importation des bibliothÃ¨ques
# 2. Configuration du parser Llama
# 3. Extraction PDF -> Markdown
# 4. CrÃ©ation de la base vectorielle
```

### Phase 2 : Lancer le Chatbot

```bash
# DÃ©marrer le serveur Streamlit
streamlit run chatbotmain.py

# Ouvrir votre navigateur Ã  l'adresse :
# http://localhost:8501
```

---

## ğŸ—ï¸ Architecture du SystÃ¨me

### Diagramme d'Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Wa7ch.pdf     â”‚â”€â”€â”€â–¶â”‚  LlamaParse     â”‚â”€â”€â”€â–¶â”‚  Wa7ch.md       â”‚
â”‚   (PDF Arabe)   â”‚    â”‚  (Extraction)   â”‚    â”‚  (Markdown)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Interface      â”‚â—€â”€â”€â”€â”‚   SystÃ¨me RAG   â”‚â—€â”€â”€â”€â”‚  ChromaDB       â”‚
â”‚  Streamlit      â”‚    â”‚   (Recherche)   â”‚    â”‚  (Vecteurs)     â”‚
â”‚  (Chatbot)      â”‚â”€â”€â”€â–¶â”‚                 â”‚â”€â”€â”€â–¶â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ollama LLM     â”‚â—€â”€â”€â”€â”‚   Embeddings    â”‚â—€â”€â”€â”€â”‚  DÃ©coupage      â”‚
â”‚  (llama3.1)     â”‚    â”‚   (mxbai)       â”‚    â”‚  (Paragraphes)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flux de DonnÃ©es

1. **Extraction** : PDF â†’ Texte structurÃ© (Markdown)
2. **PrÃ©paration** : DÃ©coupage en paragraphes â†’ Documents
3. **Embedding** : Transformation texte â†’ Vecteurs
4. **Indexation** : Stockage dans ChromaDB
5. **Recherche** : Question â†’ RÃ©cupÃ©ration de contexte
6. **GÃ©nÃ©ration** : Contexte + Question â†’ RÃ©ponse

---

## ğŸ’» Explication DÃ©taillÃ©e du Code

### Partie 1 : Notebook `dataembeddings.ipynb`

#### Ã‰tape 1 : Importation des BibliothÃ¨ques

```python
import os
from llama_parse import LlamaParse
from llama_parse.base import ResultType
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document
from llama_cloud_services.parse.utils import Language
from langchain_community.embeddings.ollama import OllamaEmbeddings
```

**Explication** :
- `LlamaParse` : Outil spÃ©cialisÃ© pour l'extraction de PDF complexes
- `Chroma` : Base de donnÃ©es vectorielle lÃ©gÃ¨re et efficace
- `OllamaEmbeddings` : ModÃ¨le local pour gÃ©nÃ©rer des embeddings
- `Language.ARABIC` : Support spÃ©cifique pour la langue arabe

#### Ã‰tape 2 : Configuration du Parser

```python
LLAMA_API_KEY = "votre_clÃ©_api"
os.environ["LLAMA_CLOUD_API_KEY"] = LLAMA_API_KEY

parser_ar = LlamaParse(
    result_type=ResultType.MD,
    language=Language.ARABIC,
    verbose=True
)
```

**Points Importants** :
- âš ï¸ **SÃ©curitÃ©** : La clÃ© API doit Ãªtre stockÃ©e dans des variables d'environnement
- `ResultType.MD` : Format Markdown pour prÃ©server la structure
- `Language.ARABIC` : Optimisation pour le traitement de l'arabe

#### Ã‰tape 3 : Extraction PDF â†’ Markdown

```python
import nest_asyncio
nest_asyncio.apply()

pdf_files = [("Wa7ch.pdf", parser_ar)]

with open("Wa7ch.md", 'w', encoding='utf-8') as f:
    for file_name, parser in pdf_files:
        documents = parser.load_data(file_name)
        for doc in documents:
            f.write(doc.text + "\n\n")
```

**Fonctionnement** :
- `nest_asyncio` : Permet l'exÃ©cution asynchrone dans Jupyter
- `load_data()` : Envoie le PDF au cloud pour traitement
- Encodage UTF-8 : Essentiel pour les caractÃ¨res arabes

#### Ã‰tape 4 : CrÃ©ation de la Base Vectorielle

```python
# 1. Lecture du fichier Markdown
with open("Wa7ch.md", encoding='utf-8') as f:
    markdown_content = f.read()

# 2. DÃ©coupage en paragraphes
paragraphs = [p.strip() for p in markdown_content.split('\n\n') if p.strip()]

# 3. CrÃ©ation des documents
documents = [Document(page_content=paragraph) for paragraph in paragraphs]

# 4. Initialisation des embeddings
embeddings = OllamaEmbeddings(model="mxbai-embed-large:latest")

# 5. CrÃ©ation de la base vectorielle
persist_directory = "philo_db"
vecdb = Chroma.from_documents(
    documents=documents,
    embedding=embeddings,
    persist_directory=persist_directory,
    collection_name="rag-chroma"
)

# 6. Persistance des donnÃ©es
vecdb.persist()
```

**DÃ©tails Techniques** :
- DÃ©coupage par `\n\n` : Simple mais efficace pour le Markdown
- `OllamaEmbeddings` : ModÃ¨le local, pas besoin d'internet aprÃ¨s tÃ©lÃ©chargement
- `persist()` : Sauvegarde sur disque pour rÃ©utilisation

### Partie 2 : Script `chatbotmain.py`

#### Configuration Initiale

```python
import ollama
import streamlit as st
from langchain.vectorstores import Chroma
from langchain_community.embeddings.ollama import OllamaEmbeddings
from langchain_ollama import OllamaLLM
from streamlit_float import *

st.set_page_config(
    page_title="Ø±ÙˆØ¨ÙˆØª Ø±ÙˆØ§ÙŠØ© ÙˆØ­Ø´",
    page_icon="ğŸ“š",
    layout="wide",
    initial_sidebar_state="collapsed"
)
```

**Composants** :
- `streamlit` : Framework pour applications web interactives
- `Chroma` : Client pour interroger la base vectorielle
- `streamlit_float` : Extension pour interface avancÃ©e

#### Interface Utilisateur

```python
# CSS personnalisÃ©
st.markdown("""
<style>
    .custom-header {
        background: linear-gradient(135deg, #2c3e50 0%, #3498db 100%);
        padding: 2rem;
        border-radius: 15px;
        margin-bottom: 2rem;
    }
    /* ... autres styles ... */
</style>
""", unsafe_allow_html=True)

# Header professionnel
st.markdown("""
<div class="custom-header">
    <h1 style="margin:0; font-size: 2.5rem; font-weight: bold;">ğŸ“š Ø±ÙˆØ¨ÙˆØª Ø±ÙˆØ§ÙŠØ© ÙˆØ­Ø´</h1>
    <p style="margin:0; font-size: 1.2rem; opacity: 0.9;">Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø£Ø³Ø¦Ù„ØªÙƒ Ø­ÙˆÙ„ Ø±ÙˆØ§ÙŠØ© "ÙˆØ­Ø´"</p>
</div>
""", unsafe_allow_html=True)
```

**Design** :
- Interface bilingue (franÃ§ais/arabe)
- Gradients et animations modernes
- Design responsive pour mobile

#### Logique RAG

```python
def retrieve_from_db(question):
    retriever = vecdb.as_retriever()
    retrieved_docs = retriever.invoke(question)
    retrieved_docs_txt = retrieved_docs[1].page_content if len(retrieved_docs) > 1 else ""
    return retrieved_docs_txt

def generate_response(user_message: str, chat_history: list=[], doc=""):
    system_msg = """Ø£Ù†Øª Ø±ÙˆØ¨ÙˆØª Ù…Ø­Ø§Ø¯Ø«Ø© Ù…ØªØ®ØµØµ ÙÙŠ Ø±ÙˆØ§ÙŠØ© "ÙˆØ­Ø´". 
    ÙŠØ¬Ø¨ Ø£Ù† ØªØ¬ÙŠØ¨ Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ø±ÙˆØ§ÙŠØ© "ÙˆØ­Ø´" Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø·.
    ÙƒÙ† Ø¯Ù‚ÙŠÙ‚Ø§Ù‹ ÙÙŠ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙˆÙˆØ§Ø¶Ø­Ø§Ù‹ ÙÙŠ Ø§Ù„Ø´Ø±Ø­.
    Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù‡Ù†Ø§Ùƒ Ø¥Ø¬Ø§Ø¨Ø© ÙƒØ§ÙÙŠØ© ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ØŒ Ø±Ø¬Ø§Ø¡Ù‹ Ø£Ø¬Ø¨ Ø¨Ù€ "Ø¢Ø³ÙØŒ Ù„Ø§ Ø£Ù…Ù„Ùƒ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„".
    """
    
    my_message = [{"role": "system", "content": system_msg.format(document=doc, question=user_message)}]
    
    for chat in chat_history:                      
        my_message.append({"role": chat["name"], "content": chat["msg"]})
    
    my_message.append({"role": "user", "content": user_message})

    response = ollama.chat(                      
        model="llama3.1",
        messages=my_message
    ) 
    return response["message"]["content"]
```

**Fonctionnement RAG** :
1. **Retrieval** : Trouve les paragraphes pertinents dans ChromaDB
2. **Contextualisation** : Combine question + contexte extrait
3. **GÃ©nÃ©ration** : Utilise Ollama pour produire une rÃ©ponse naturelle

#### Gestion de l'Ã‰tat

```python
def main():
    if "chat_log" not in st.session_state:
        st.session_state.chat_log = []
    
    # Interface utilisateur
    col1, col2 = st.columns([3, 1])
    
    with col1:
        chat_container = st.container()
        # Affichage de l'historique
    
    with col2:
        # Questions suggÃ©rÃ©es
        suggested_questions = [
            "Ù…Ø§ Ù‡ÙŠ Ù‚ØµØ© Ø±ÙˆØ§ÙŠØ© ÙˆØ­Ø´ØŸ",
            "Ù…Ù† Ù‡Ùˆ Ø¨Ø·Ù„ Ø§Ù„Ø±ÙˆØ§ÙŠØ©ØŸ",
            "Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ÙÙŠ Ø§Ù„Ø±ÙˆØ§ÙŠØ©ØŸ"
        ]
    
    # Zone de saisie
    user_message = st.chat_input("Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ø¹Ù† Ø±ÙˆØ§ÙŠØ© ÙˆØ­Ø´ Ù‡Ù†Ø§...")
    
    # Traitement
    if user_message:
        doc = retrieve_from_db(user_message)
        response = generate_response(user_message, chat_history=st.session_state.chat_log, doc=doc)
        
        # Mise Ã  jour de l'historique
        st.session_state.chat_log.append({"name": "user", "msg": user_message})
        st.session_state.chat_log.append({"name": "assistant", "msg": response})
```

**Session State** :
- Persistance de l'historique pendant la session
- Gestion asynchrone des interactions
- Mise Ã  jour en temps rÃ©el

---

## âš™ï¸ Configuration

### Variables d'Environnement

```bash
# .env file
LLAMA_CLOUD_API_KEY=votre_clÃ©_api_llama
OLLAMA_HOST=http://localhost:11434
```

### ModÃ¨les Ollama Requis

```bash
# Embeddings
ollama pull mxbai-embed-large:latest

# ModÃ¨le de langage
ollama pull llama3.1:latest

# VÃ©rification
ollama list
```

### Configuration ChromaDB

```python
# Dans le notebook
persist_directory = "philo_db"  # Chemin local
collection_name = "rag-chroma"  # Nom de la collection

# Dans le chatbot
persist_directory = "rag/philo_db"  # Chemin alternatif
```

---

## ğŸ”§ DÃ©pannage

### ProblÃ¨mes Courants

#### 1. Erreur : "LlamaParse API key not found"
```bash
Solution:
export LLAMA_CLOUD_API_KEY="votre_clÃ©"
# Ou ajouter Ã  .env
```

#### 2. Ollama ne rÃ©pond pas
```bash
# VÃ©rifier le service
ollama serve

# VÃ©rifier les modÃ¨les
ollama list

# RedÃ©marrer
pkill ollama
ollama serve
```

#### 3. Encodage arabe incorrect
```python
# Ajouter encoding='utf-8' Ã  tous les open()
with open("fichier.md", 'r', encoding='utf-8') as f:
    content = f.read()
```

#### 4. Erreur de dÃ©pendances
```bash
# Mettre Ã  jour pip
pip install --upgrade pip

# RÃ©installer les dÃ©pendances
pip install -r requirements.txt --force-reinstall
```

#### 5. Streamlit ne se lance pas
```bash
# VÃ©rifier le port
streamlit run chatbotmain.py --server.port 8501

# DÃ©sactiver le cache
streamlit run chatbotmain.py --server.fileWatcherType none
```

### Logs de DÃ©bogage

```python
# Activer les logs dÃ©taillÃ©s
import logging
logging.basicConfig(level=logging.DEBUG)

# VÃ©rifier la connexion Ollama
import requests
response = requests.get("http://localhost:11434/api/tags")
print(response.json())
```

---

## ğŸ“ Contact et Support

### DÃ©veloppeur Principal

**Nom complet** : Oussama fahim  
**Email** : Oussamafahim2017@gmail.com  
**TÃ©lÃ©phone** : +212 645468306 

---

## ğŸ¤ Contribution

### Comment Contribuer

1. **Fork** le repository
2. **CrÃ©er une branche** (`git checkout -b feature/nouvelle-fonctionnalite`)
3. **Commit** les changements (`git commit -am 'Ajout nouvelle fonctionnalitÃ©'`)
4. **Push** sur la branche (`git push origin feature/nouvelle-fonctionnalite`)
5. **CrÃ©er une Pull Request**

### Zones d'AmÃ©lioration

- ğŸŒ **Support multilingue** (franÃ§ais, anglais)
- ğŸ“Š **Analytiques d'utilisation** du chatbot
- ğŸ¨ **ThÃ¨mes personnalisables** pour l'interface
- ğŸ” **AmÃ©lioration de la recherche** sÃ©mantique
- ğŸ“± **Application mobile** native

---

## ğŸ“š Ressources SupplÃ©mentaires

### Documentation Officielle

- [LangChain Documentation](https://python.langchain.com/)
- [Streamlit Documentation](https://docs.streamlit.io/)
- [Ollama Documentation](https://github.com/ollama/ollama)
- [ChromaDB Documentation](https://docs.trychroma.com/)

### Tutoriels RecommandÃ©s

- [Introduction au RAG](https://python.langchain.com/docs/use_cases/question_answering/)
- [Traitement du Langage Naturel en Arabe](https://github.com/aub-mind/arabert)
- [DÃ©ploiement d'Applications Streamlit](https://streamlit.io/cloud)

---
### Technologies UtilisÃ©es

- **LlamaParse** pour l'extraction robuste de PDF
- **Ollama** pour l'exÃ©cution locale de modÃ¨les
- **ChromaDB** pour le stockage vectoriel efficace
- **Streamlit** pour l'interface utilisateur intuitive

### Inspiration

- CommunautÃ© open-source des modÃ¨les de langage
- Projets Ã©ducatifs en traitement automatique de l'arabe
- Innovations rÃ©centes en systÃ¨mes RAG

---

<div align="center">

## â­ Supportez le Projet

Si ce projet vous a Ã©tÃ© utile, pensez Ã  lui donner une Ã©toile sur GitHub !

**DÃ©veloppÃ© avec â¤ï¸ par oussama fahim**

</div>
