{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12e6b48c",
   "metadata": {},
   "source": [
    "## Ce code met en place la première partie d'un système RAG (Retrieval-Augmented Generation) : l'extraction de données et la création d'une base de données vectorielle.\n",
    "## ⚠️ Avertissement de sécurité :  \n",
    " Vous avez laissé votre clé API (LLAMA_API_KEY) visible dans le code. Je vous conseille vivement de la révoquer et d'en générer une nouvelle, car n'importe qui ayant accès à ce texte peut l'utiliser.\n",
    "\n",
    " ### 1. Analyse détaillée par étape\n",
    "\n",
    "Étape 1 : Importation des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce08d310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_parse import LlamaParse\n",
    "from llama_parse.base import ResultType\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from llama_cloud_services.parse.utils import Language\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd452725",
   "metadata": {},
   "source": [
    "### Explication :\n",
    " Ce bloc charge tous les outils nécessaires.\n",
    "LlamaParse : Un outil puissant pour extraire du texte depuis des fichiers complexes (PDF), particulièrement bon pour lire les tableaux et les structures.\n",
    "Chroma : La base de données vectorielle qui va stocker les informations.\n",
    "OllamaEmbeddings : Le modèle qui va transformer le texte en vecteurs (listes de nombres) pour que la machine comprenne le sens des phrases.\n",
    "### Interprétation :\n",
    " On prépare la \"boîte à outils\". On choisit ici d'utiliser Ollama (local) pour les embeddings et Chroma pour le stockage, ce qui permet une architecture privée et efficace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a5d1c3",
   "metadata": {},
   "source": [
    "Étape 2 : Configuration du Parser Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44c27a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Parser arabe initialisé avec succès\n"
     ]
    }
   ],
   "source": [
    "# Configuration de l'API Key\n",
    "LLAMA_API_KEY = \"llx-a2C7FgYfP1hzX3pXuvtdaNmexAqsuRnJIJ2G6MjbBrfuS3QY\"  # Remplacez par votre clé\n",
    "os.environ[\"LLAMA_CLOUD_API_KEY\"] = LLAMA_API_KEY\n",
    "\n",
    "# Initialisation du parser pour l'arabe\"\n",
    "parser_ar = LlamaParse(\n",
    "    result_type=ResultType.MD,\n",
    "    language=Language.ARABIC,\n",
    "    verbose=True)\n",
    "print(\"✅ Parser arabe initialisé avec succès\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676fab15",
   "metadata": {},
   "source": [
    "### Explication : \n",
    "On configure la clé API pour le service LlamaCloud. On initialise ensuite le parser en spécifiant que le document est en Arabe (Language.ARABIC) et qu'on veut le résultat au format Markdown (ResultType.MD).\n",
    "### Interprétation : \n",
    "C'est une étape critique pour votre document \"Wa7ch.pdf\". Les PDF en arabe sont souvent très mal lus par les extracteurs classiques. LlamaParse utilise de l'IA pour reconstruire correctement le texte arabe et la structure du document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7186b17",
   "metadata": {},
   "source": [
    "Étape 3 : Extraction et Conversion (PDF -> Markdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3a22113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement de Wa7ch.pdf...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 3bef5486-f45e-495f-acac-2601b268a6fd\n",
      ".✅ Tous les textes ont été extraits et sauvegardés dans : Wa7ch.md\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "pdf_files = [\n",
    "    (\"Wa7ch.pdf\", parser_ar)\n",
    "   \n",
    "    \n",
    "    # ajoute autant que nécessaire\n",
    "]\n",
    "\n",
    "# Nom du fichier Markdown de sortie\n",
    "output_filename = \"Wa7ch.md\"\n",
    "\n",
    "# Traitement et sauvegarde\n",
    "with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "    for file_name, parser in pdf_files:\n",
    "        print(f\"Traitement de {file_name}...\")\n",
    "        documents = parser.load_data(file_name)\n",
    "        f.write(f\"# Contenu extrait de : {file_name}\\n\\n\")\n",
    "        for doc in documents:\n",
    "            f.write(doc.text + \"\\n\\n\")\n",
    "\n",
    "print(f\"✅ Tous les textes ont été extraits et sauvegardés dans : {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57698ff5",
   "metadata": {},
   "source": [
    "### Explication :\n",
    "nest_asyncio est utilisé car LlamaParse fonctionne de manière asynchrone, ce qui peut poser problème dans un notebook Jupyter sans ce correctif.\n",
    "Le code envoie le PDF au cloud, récupère le texte structuré et le sauvegarde dans un fichier local Wa7ch.md.\n",
    "### Interprétation :\n",
    " On transforme une donnée \"binaire\" et illisible pour un LLM (le PDF) en un format texte structuré (Markdown) qui préserve les titres et paragraphes. C'est l'étape de Digitalisation/Extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed3367f",
   "metadata": {},
   "source": [
    "Étape 4 : Création de la Base Vectorielle (Ingestion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d7bb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hiba\\AppData\\Local\\Temp\\ipykernel_21892\\821983471.py:12: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model=\"mxbai-embed-large:latest\")\n"
     ]
    }
   ],
   "source": [
    "# 1. Lecture du fichier Markdown\n",
    "with open(\"Wa7ch.md\", encoding='utf-8') as f:\n",
    "    markdown_content = f.read()\n",
    "\n",
    "# 2. Découpage par paragraphes (2 sauts de ligne ou plus)\n",
    "paragraphs = [p.strip() for p in markdown_content.split('\\n\\n') if p.strip()]\n",
    "\n",
    "# 3. Création des documents\n",
    "documents = [Document(page_content=paragraph) for paragraph in paragraphs]\n",
    "\n",
    "# 4. Initialisation des embeddings\n",
    "embeddings = OllamaEmbeddings(model=\"mxbai-embed-large:latest\")\n",
    "\n",
    "# 5. Configuration de la base vectorielle\n",
    "persist_directory = \"philo_db\"\n",
    "vecdb = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory,\n",
    "    collection_name=\"rag-chroma\"\n",
    ")\n",
    "\n",
    "# 6. Persistance des données\n",
    "vecdb.persist()\n",
    "print(\"Opération terminée avec succès:\")\n",
    "print(f\"- {len(documents)} paragraphes traités\")\n",
    "print(f\"- Base vectorielle sauvegardée dans: {persist_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85120f18",
   "metadata": {},
   "source": [
    "### Explication :\n",
    "On relit le fichier Markdown créé juste avant.\n",
    "On découpe le texte par paragraphes (double saut de ligne \\n\\n). Chaque paragraphe devient un \"Document\".\n",
    "On charge le modèle d'embedding mxbai-embed-large via Ollama (qui doit tourner localement sur votre machine).\n",
    "On crée la base de données philo_db qui contient les vecteurs de ces paragraphes.\n",
    "### Interprétation : \n",
    "C'est l'étape d'Indexation. Le texte est transformé en coordonnées mathématiques. Plus tard, quand vous poserez une question, le système comparera les coordonnées de votre question avec celles de ces paragraphes pour trouver la réponse (Recherche Sémantique)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b64e10",
   "metadata": {},
   "source": [
    "Le flux de travail est organisé de manière linéaire et logique pour un pipeline ETL (Extract, Transform, Load) :\n",
    "### Configuration (Setup) :\n",
    " Clés API et initialisation des objets.\n",
    "### Extraction (Extract) : \n",
    "Envoi du PDF brut -> Réception du Markdown propre (grâce à LlamaParse).\n",
    "### Transformation (Transform) :\n",
    " Découpage du Markdown en petits morceaux (chunks/paragraphes).\n",
    "### Chargement & Indexation (Load) :\n",
    " Calcul des vecteurs (Embeddings) et sauvegarde dans la base de données (ChromaDB)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
